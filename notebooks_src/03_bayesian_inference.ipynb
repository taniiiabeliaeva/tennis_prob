{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0035f9",
   "metadata": {},
   "source": [
    "\n",
    "Here we quantify the impact of serve performance on point outcomes using a Bayesian Logistic Regression model.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1.  **Preprocessing**: Calculate *deviations* from historical baselines (e.g., \"Is Sinner serving better than his 2023 average?\").\n",
    "2.  **Probabilistic Model**:\n",
    "      * **Likelihood**: Bernoulli (Point Won / Lost).\n",
    "      * **Link Function**: Logit (maps linear predictors to 0-1 probability).\n",
    "      * **Priors**: Normal distributions for intercept and coefficients.\n",
    "3.  **Inference**: Use the NUTS (No-U-Turn Sampler) algorithm to generate posterior samples.\n",
    "4.  **Diagnostics**: Verify convergence using $\\hat{R}$ and Effective Sample Size (ESS).\n",
    "\n",
    "Model Specification\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_i &\\sim \\text{Bernoulli}(\\theta_i) \\\\\n",
    "\\text{logit}(\\theta_i) &= \\alpha_{\\text{player}[i]} + \\beta_{srv} \\cdot (X_{srv, i} - \\mu_{srv, hist}) + \\beta_{ace} \\cdot (X_{ace, i} - \\mu_{ace, hist})\n",
    "\\end{align*}\n",
    "\n",
    "$$Where predictors are on the natural 0-1 scale to maintain interpretability without z-scoring artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ee763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent \n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64526814",
   "metadata": {},
   "source": [
    "## Load & Preprocess Data\n",
    "\n",
    "We load the feature matrix from Notebook 02. Crucially, we create **Deviation Features** ($X - X_{hist}$) to center our predictors. This ensures the model intercept ($\\beta_0$) represents the baseline win probability when players are performing at their historical averages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23938b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 326 observation points.\n",
      "\n",
      "--- Data Vectors Prepared ---\n",
      "Server ID shape: (326,)\n",
      "Serve Deviation range: [-0.278, 0.155]\n",
      "Ace Deviation range:   [-0.143, 0.029]\n"
     ]
    }
   ],
   "source": [
    "# Load Features and Priors\n",
    "feature_path = PROCESSED_DIR / 'features_match_1501.pkl'\n",
    "features_df = pd.read_pickle(feature_path)\n",
    "\n",
    "print(f\"Loaded {len(features_df)} observation points.\")\n",
    "\n",
    "# Extract Arrays for PyMC\n",
    "# 0 = P1 (Sinner), 1 = P2 (Medvedev)\n",
    "server_id = features_df['server_id'].values\n",
    "y = features_df['y'].values\n",
    "\n",
    "# Features on natural 0-1 scale\n",
    "srv_pct = features_df['srv_pct'].values\n",
    "ace_rate = features_df['ace_rate'].values\n",
    "hist_srv_pct = features_df['hist_srv_pct'].values\n",
    "hist_ace_rate = features_df['hist_ace_rate'].values\n",
    "\n",
    "# Calculate Deviations (Centering)\n",
    "srv_deviation = srv_pct - hist_srv_pct\n",
    "ace_deviation = ace_rate - hist_ace_rate\n",
    "\n",
    "print(\"\\n--- Data Vectors Prepared ---\")\n",
    "print(f\"Server ID shape: {server_id.shape}\")\n",
    "print(f\"Serve Deviation range: [{srv_deviation.min():.3f}, {srv_deviation.max():.3f}]\")\n",
    "print(f\"Ace Deviation range:   [{ace_deviation.min():.3f}, {ace_deviation.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55190653",
   "metadata": {},
   "source": [
    "## Full Model Inference\n",
    "\n",
    "We first fit the model on the complete dataset (all points) to establish the final posterior estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha_player, beta_srv, beta_ace]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d62168266a46629cb4fc783c3b85a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coords = {\"player\": [\"Player 1\", \"Player 2\"]}\n",
    "\n",
    "with pm.Model(coords=coords) as tennis_model:\n",
    "    # --- Priors ---\n",
    "    # Intercepts: Player-specific baseline win probability\n",
    "    alpha_player = pm.Normal(\"alpha_player\", mu=0, sigma=1.5, dims=\"player\")\n",
    "    \n",
    "    # Slopes: Effect of technical deviations\n",
    "    # Sigma=2 allows for wide range of effects on logit scale\n",
    "    beta_srv = pm.Normal(\"beta_srv\", mu=0, sigma=2)\n",
    "    beta_ace = pm.Normal(\"beta_ace\", mu=0, sigma=3)\n",
    "    \n",
    "    # --- Linear Predictor ---\n",
    "    logit_theta = (alpha_player[server_id] + \n",
    "                   beta_srv * srv_deviation + \n",
    "                   beta_ace * ace_deviation)\n",
    "    \n",
    "    # --- Likelihood ---\n",
    "    # Inverse logit transforms to probability space [0, 1]\n",
    "    theta = pm.Deterministic(\"theta\", pm.math.invlogit(logit_theta))\n",
    "    y_obs = pm.Bernoulli(\"y_obs\", p=theta, observed=y)\n",
    "\n",
    "    # --- Sampling ---\n",
    "    trace = pm.sample(\n",
    "        draws=2000, \n",
    "        tune=1000, \n",
    "        chains=4, \n",
    "        cores=4, \n",
    "        target_accept=0.95,\n",
    "        return_inferencedata=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e45a7c",
   "metadata": {},
   "source": [
    "## Diagnostics & Results\n",
    "\n",
    "We check the health of the MCMC chains using:\n",
    "\n",
    "1.  **Trace Plot**: Should look like \"fuzzy caterpillars\" (good mixing).\n",
    "2.  **$\\hat{R}$ (R-hat)**: Should be $< 1.01$ (indicates convergence).\n",
    "3.  **ESS (Effective Sample Size)**: Should be $> 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f88ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Summary Statistics\n",
    "summary = az.summary(trace, var_names=[\"alpha\", \"beta_srv\", \"beta_ace\"])\n",
    "print(summary)\n",
    "\n",
    "# 2. Trace Plot\n",
    "az.plot_trace(trace, var_names=[\"alpha\", \"beta_srv\", \"beta_ace\"], compact=False)\n",
    "plt.savefig(PLOTS_DIR / '03_trace_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Posterior Densities (Forest Plot)\n",
    "# Visualizes the 94% HDI (Highest Density Interval)\n",
    "az.plot_forest(trace, var_names=[\"alpha\", \"beta_srv\", \"beta_ace\"], combined=True, hdi_prob=0.94)\n",
    "plt.title(\"Posterior Distributions: Parameter Estimates\")\n",
    "plt.savefig(PLOTS_DIR / '03_posterior_forest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c098f",
   "metadata": {},
   "source": [
    "## Sequential Inference (Key Innovation)\n",
    "To answer the research question regarding uncertainty reduction over time, we refit the model at incremental windows (e.g., after 10 points, 20 points, etc.). This allows us to track the evolution of the coefficients and their credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb695907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define windows for sequential fitting\n",
    "windows = [10, 20, 50, 100, 150, 200, 250, len(y)]\n",
    "traces_over_time = []\n",
    "\n",
    "print(f\"Starting sequential inference for {len(windows)} windows...\")\n",
    "\n",
    "for w in windows:\n",
    "    print(f\"  Fitting model with first {w} points...\")\n",
    "    \n",
    "    # Subset Data\n",
    "    server_sub = server_id[:w]\n",
    "    srv_dev_sub = srv_deviation[:w]\n",
    "    ace_dev_sub = ace_deviation[:w]\n",
    "    y_sub = y[:w]\n",
    "    \n",
    "    with pm.Model() as model_t:\n",
    "        # Priors (same as full model)\n",
    "        alpha_t = pm.Normal(\"alpha_player\", mu=0, sigma=1.5, shape=2)\n",
    "        beta_srv_t = pm.Normal(\"beta_srv\", mu=0, sigma=2)\n",
    "        beta_ace_t = pm.Normal(\"beta_ace\", mu=0, sigma=3)\n",
    "        \n",
    "        # Likelihood\n",
    "        logit_t = alpha_t[server_sub] + beta_srv_t * srv_dev_sub + beta_ace_t * ace_dev_sub\n",
    "        y_obs_t = pm.Bernoulli(\"y_obs\", p=pm.math.invlogit(logit_t), observed=y_sub)\n",
    "        \n",
    "        # Efficient Sampling for iterative steps\n",
    "        # Reduced chains/draws for intermediate steps to save time\n",
    "        trace_t = pm.sample(draws=1000, tune=1000, chains=2, progressbar=False)\n",
    "        traces_over_time.append(trace_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2060c9",
   "metadata": {},
   "source": [
    "## Coefficient Evolution Analysis\n",
    "We extract the mean and 90% credible intervals (quantile 0.05 to 0.95) for each parameter across all time windows. This visualizes how the model \"learns\" the game dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f02c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage\n",
    "beta_srv_evo, beta_ace_evo = [], []\n",
    "alpha_p1_evo, alpha_p2_evo = [], []\n",
    "\n",
    "# Extract stats from traces\n",
    "for t in traces_over_time:\n",
    "    # Helper to get stats\n",
    "    def get_stats(param_name, sel_idx=None):\n",
    "        post = t.posterior[param_name]\n",
    "        if sel_idx is not None:\n",
    "            post = post.sel(alpha_player_dim_0=sel_idx)\n",
    "        return {\n",
    "            'mean': float(post.mean()),\n",
    "            'q5': float(post.quantile(0.05)),\n",
    "            'q95': float(post.quantile(0.95))\n",
    "        }\n",
    "\n",
    "    beta_srv_evo.append(get_stats('beta_srv'))\n",
    "    beta_ace_evo.append(get_stats('beta_ace'))\n",
    "    alpha_p1_evo.append(get_stats('alpha_player', 0))\n",
    "    alpha_p2_evo.append(get_stats('alpha_player', 1))\n",
    "\n",
    "# Plotting the 2x2 Grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "def plot_evolution(ax, data, title, ylabel, color):\n",
    "    means = [d['mean'] for d in data]\n",
    "    q5 = [d['q5'] for d in data]\n",
    "    q95 = [d['q95'] for d in data]\n",
    "    \n",
    "    ax.plot(windows, means, marker='o', linewidth=2, color=color, label='Mean')\n",
    "    ax.fill_between(windows, q5, q95, alpha=0.3, color=color, label='90% HDI')\n",
    "    ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel('Points Observed')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 1. Beta Serve\n",
    "plot_evolution(axes[0, 0], beta_srv_evo, 'β_srv (First Serve Effect)', 'Coefficient Value', 'tab:blue')\n",
    "\n",
    "# 2. Beta Ace\n",
    "plot_evolution(axes[0, 1], beta_ace_evo, 'β_ace (Ace Rate Effect)', 'Coefficient Value', 'tab:orange')\n",
    "\n",
    "# 3. Alpha P1\n",
    "plot_evolution(axes[1, 0], alpha_p1_evo, 'α_P1 (Baseline Log-Odds)', 'Log-Odds', 'tab:green')\n",
    "\n",
    "# 4. Alpha P2\n",
    "plot_evolution(axes[1, 1], alpha_p2_evo, 'α_P2 (Baseline Log-Odds)', 'Log-Odds', 'tab:purple')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / '03_coefficient_evolution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984aeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the full InferenceData object (NetCDF format) for the final reporting notebook.\n",
    "\n",
    "trace.to_netcdf(RESULTS_DIR / 'trace_final.nc')\n",
    "\n",
    "evolution_data = {\n",
    "    'windows': windows,\n",
    "    'beta_srv': beta_srv_evo,\n",
    "    'beta_ace': beta_ace_evo,\n",
    "    'alpha_p1': alpha_p1_evo,\n",
    "    'alpha_p2': alpha_p2_evo\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'coefficient_evolution.pkl', 'wb') as f:\n",
    "    pickle.dump(evolution_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
